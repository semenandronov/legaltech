"""Assistant UI chat endpoint for streaming responses"""
from fastapi import APIRouter, HTTPException, Depends, Request, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session
from typing import AsyncGenerator, Optional, Literal
import hashlib
from app.utils.database import get_db
from app.utils.auth import get_current_user
from app.models.case import Case, File as FileModel, ChatMessage
from app.models.user import User
from app.services.rag_service import RAGService
from app.services.document_processor import DocumentProcessor
from app.services.langchain_memory import MemoryService
from app.services.llm_factory import create_llm, create_legal_llm
# Agents removed - using simple RAG chat only
from app.services.external_sources.web_research_service import get_web_research_service
from app.services.external_sources.source_router import get_source_router, initialize_source_router
from app.services.external_sources.cache_manager import get_cache_manager
from app.services.langchain_agents.pipeline_service import PipelineService
from app.services.langchain_agents.planning_agent import PlanningAgent
from app.services.langchain_agents.advanced_planning_agent import AdvancedPlanningAgent
from app.config import config
import json
import logging
import asyncio
from datetime import datetime

logger = logging.getLogger(__name__)

router = APIRouter()

# Initialize services
rag_service = RAGService()
document_processor = DocumentProcessor()
memory_service = MemoryService()

# Initialize classification cache
_classification_cache = None


def get_classification_cache():
    """Get or create classification cache manager"""
    global _classification_cache
    if _classification_cache is None:
        from app.config import config
        redis_url = getattr(config, 'REDIS_URL', None)
        ttl = getattr(config, 'CACHE_TTL_SECONDS', 3600)
        _classification_cache = get_cache_manager(redis_url=redis_url, default_ttl=ttl)
    return _classification_cache


class AssistantMessage(BaseModel):
    """Message model for assistant-ui"""
    role: str = Field(..., description="Message role: user or assistant")
    content: str = Field(..., description="Message content")


class ClassificationResult(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    label: Literal["task", "question"] = Field(..., description="–ú–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: task –∏–ª–∏ question")
    confidence: float = Field(..., ge=0.0, le=1.0, description="–£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ç 0.0 –¥–æ 1.0")
    rationale: Optional[str] = Field(None, description="–ö—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")


# Note: Request body is parsed manually to support assistant-ui format
# Assistant-ui sends: { messages: [...], case_id: "..." }


def normalize_text(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è: lower, strip, —É–±–∏—Ä–∞–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    return " ".join(text.lower().strip().split())


def make_classification_cache_key(question: str) -> str:
    """
    –°–æ–∑–¥–∞–µ—Ç cache key –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    
    Args:
        question: –í—Ö–æ–¥–Ω–æ–π –≤–æ–ø—Ä–æ—Å
        
    Returns:
        Cache key (—Ö–µ—à –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞)
    """
    normalized = normalize_text(question)
    key_hash = hashlib.sha256(normalized.encode('utf-8')).hexdigest()
    return f"classification:{key_hash}"


async def classify_request(question: str, llm) -> bool:
    """
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∑–∞–¥–∞—á–µ–π –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–æ–≤
    –∏–ª–∏ –æ–±—ã—á–Ω—ã–º –≤–æ–ø—Ä–æ—Å–æ–º –¥–ª—è RAG —á–∞—Ç–∞.
    
    Args:
        question: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        llm: LLM –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    
    Returns:
        True –µ—Å–ª–∏ —ç—Ç–æ –∑–∞–¥–∞—á–∞, False –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å
    """
    import re
    
    # 1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    normalized_question = normalize_text(question)
    question_lower = normalized_question.lower()
    
    # 2. Rule-based –ø—Ä–æ–≤–µ—Ä–∫–∞ (fast path)
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å—Ç–∞—Ç–µ–π –∫–æ–¥–µ–∫—Å–æ–≤ - –≤—Å–µ–≥–¥–∞ QUESTION
    article_patterns = [
        r'—Å—Ç–∞—Ç—å—è\s+\d+\s+(–≥–ø–∫|–≥–∫|–∞–ø–∫|—É–∫|–Ω–∫|—Ç–∫|—Å–∫|–∂–∫|–∑–∫–ø–ø|–∫–∞—Å)',
        r'\d+\s+—Å—Ç–∞—Ç—å—è\s+(–≥–ø–∫|–≥–∫|–∞–ø–∫|—É–∫|–Ω–∫|—Ç–∫|—Å–∫|–∂–∫|–∑–∫–ø–ø|–∫–∞—Å)',
        r'—Å—Ç–∞—Ç—å—è\s+\d+\s+(–≥—Ä–∞–∂–¥–∞–Ω—Å–∫|–∞—Ä–±–∏—Ç—Ä–∞–∂|—É–≥–æ–ª–æ–≤–Ω|–Ω–∞–ª–æ–≥–æ–≤|—Ç—Ä—É–¥–æ–≤|—Å–µ–º–µ–π–Ω|–∂–∏–ª–∏—â–Ω|–∑–µ–º–µ–ª—å–Ω|–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω)',
        r'–ø—Ä–∏—à–ª–∏\s+—Å—Ç–∞—Ç—å—é',
        r'–ø–æ–∫–∞–∂–∏\s+—Å—Ç–∞—Ç—å—é',
        r'–Ω–∞–π–¥–∏\s+—Å—Ç–∞—Ç—å—é',
        r'—Ç–µ–∫—Å—Ç\s+—Å—Ç–∞—Ç—å–∏',
    ]
    
    for pattern in article_patterns:
        if re.search(pattern, question_lower):
            logger.info(f"Pre-classified '{question[:50]}...' as QUESTION (matches article request pattern: {pattern})")
            return False
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π - –≤—Å–µ–≥–¥–∞ QUESTION
    greeting_patterns = [
        r'^(–ø—Ä–∏–≤–µ—Ç|–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π|–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ|–¥–æ–±—Ä—ã–π\s+(–¥–µ–Ω—å|–≤–µ—á–µ—Ä|—É—Ç—Ä–æ)|hello|hi)',
    ]
    
    for pattern in greeting_patterns:
        if re.search(pattern, question_lower):
            logger.info(f"Pre-classified '{question[:50]}...' as QUESTION (matches greeting pattern: {pattern})")
            return False
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
    cache = get_classification_cache()
    cached_result = cache.get("classification", normalized_question)
    
    if cached_result:
        label = cached_result.get("label", "question")
        cached_confidence = cached_result.get("confidence", 1.0)
        logger.info(f"Cache hit for classification: '{question[:50]}...' -> {label} (confidence: {cached_confidence:.2f})")
        return label == "task"
    
    # 4. LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å structured output
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    from app.services.langchain_agents.planning_tools import AVAILABLE_ANALYSES
    
    agents_list = []
    for agent_name, agent_info in AVAILABLE_ANALYSES.items():
        description = agent_info["description"]
        keywords = ", ".join(agent_info["keywords"][:3])  # –ü–µ—Ä–≤—ã–µ 3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
        agents_list.append(f"- {agent_name}: {description} (–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords})")
    
    agents_text = "\n".join(agents_list)
    
    # Few-shot –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    few_shot_examples = [
        (HumanMessage(content="–ó–∞–ø—Ä–æ—Å: –ò–∑–≤–ª–µ–∫–∏ –≤—Å–µ –¥–∞—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"), 
         AIMessage(content='{"label": "task", "confidence": 0.95, "rationale": "–¢—Ä–µ–±—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ entity_extraction"}')),
        (HumanMessage(content="–ó–∞–ø—Ä–æ—Å: –ö–∞–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å—Ä–æ–∫–∏ –≤–∞–∂–Ω—ã –≤ —ç—Ç–æ–º –¥–µ–ª–µ?"), 
         AIMessage(content='{"label": "question", "confidence": 0.98, "rationale": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å –¥–ª—è RAG —á–∞—Ç–∞"}')),
        (HumanMessage(content="–ó–∞–ø—Ä–æ—Å: –ü—Ä–∏—à–ª–∏ —Å—Ç–∞—Ç—å—é 135 –ì–ü–ö"), 
         AIMessage(content='{"label": "question", "confidence": 0.99, "rationale": "–ó–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ –∫–æ–¥–µ–∫—Å–∞"}')),
        (HumanMessage(content="–ó–∞–ø—Ä–æ—Å: –ù–∞–π–¥–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"), 
         AIMessage(content='{"label": "task", "confidence": 0.92, "rationale": "–¢—Ä–µ–±—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ discrepancy"}')),
        (HumanMessage(content="–ó–∞–ø—Ä–æ—Å: –ß—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç—Å—è –≤ –¥–æ–≥–æ–≤–æ—Ä–µ –æ —Å—Ä–æ–∫–∞—Ö?"), 
         AIMessage(content='{"label": "question", "confidence": 0.96, "rationale": "–í–æ–ø—Ä–æ—Å –æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è RAG"}')),
        (HumanMessage(content="–ó–∞–ø—Ä–æ—Å: –°–æ—Å—Ç–∞–≤—å —Ç–∞–±–ª–∏—Ü—É —Å —Å—É–¥—å—è–º–∏ –∏ —Å—É–¥–∞–º–∏"), 
         AIMessage(content='{"label": "task", "confidence": 0.94, "rationale": "–¢—Ä–µ–±—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–æ–≤"}')),
    ]
    
    system_content = f"""–¢—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —Å–∏—Å—Ç–µ–º–µ –∞–Ω–∞–ª–∏–∑–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–í —Å–∏—Å—Ç–µ–º–µ –¥–æ—Å—Ç—É–ø–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á:

{agents_text}

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã:
- document_classifier: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–¥–æ–≥–æ–≤–æ—Ä/–ø–∏—Å—å–º–æ/–ø—Ä–∏–≤–∏–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
- entity_extraction: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (–∏–º–µ–Ω–∞, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, —Å—É–º–º—ã, –¥–∞—Ç—ã)
- privilege_check: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–≤–∏–ª–µ–≥–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞:

–ó–ê–î–ê–ß–ê (task) - –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤:
- –ó–∞–ø—Ä–æ—Å –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —Ñ—É–Ω–∫—Ü–∏—è–º –∞–≥–µ–Ω—Ç–æ–≤ (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç, –ø–æ–∏—Å–∫ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π, –∞–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ –∏ —Ç.–¥.)
- –¢—Ä–µ–±—É–µ—Ç –∑–∞–ø—É—Å–∫–∞ —Ñ–æ–Ω–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–æ–≤
- –ù–ï –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –ø—Ä–æ—Å—Ç—ã–º –∑–∞–ø—Ä–æ—Å–∞–º –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (—Å—Ç–∞—Ç—å–∏ –∫–æ–¥–µ–∫—Å–æ–≤, —Ç–µ–∫—Å—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
- –ü—Ä–∏–º–µ—Ä—ã: "–ò–∑–≤–ª–µ–∫–∏ –≤—Å–µ –¥–∞—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "–ù–∞–π–¥–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è", "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä–∏—Å–∫–∏", "–°–æ–∑–¥–∞–π —Ä–µ–∑—é–º–µ –¥–µ–ª–∞", "—Å–æ—Å—Ç–∞–≤—å —Ç–∞–±–ª–∏—Ü—É —Å —Å—É–¥—å—è–º–∏ –∏ —Å—É–¥–∞–º–∏"

–í–û–ü–†–û–° (question) - –µ—Å–ª–∏ —ç—Ç–æ –æ–±—ã—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å –¥–ª—è RAG —á–∞—Ç–∞:
- –í–æ–ø—Ä–æ—Å—ã —Å "–∫–∞–∫–∏–µ", "—á—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–∫—Ç–æ", "–ø–æ—á–µ–º—É"
- –†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ —Ñ—Ä–∞–∑—ã: "–∫–∞–∫ –¥–µ–ª–∞", "–ø—Ä–∏–≤–µ—Ç"
- –ó–∞–ø—Ä–æ—Å—ã –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (—Å—Ç–∞—Ç—å–∏ –∫–æ–¥–µ–∫—Å–æ–≤, –Ω–æ—Ä–º –ø—Ä–∞–≤–∞, —Ç–µ–∫—Å—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
- –¢—Ä–µ–±—É–µ—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
- –ü—Ä–∏–º–µ—Ä—ã: "–ö–∞–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å—Ä–æ–∫–∏ –≤–∞–∂–Ω—ã –≤ —ç—Ç–æ–º –¥–µ–ª–µ?", "–ß—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç—Å—è –≤ –¥–æ–≥–æ–≤–æ—Ä–µ –æ —Å—Ä–æ–∫–∞—Ö?", "–ü—Ä–∏—à–ª–∏ —Å—Ç–∞—Ç—å—é 135 –ì–ü–ö", "–ü–æ–∫–∞–∂–∏ —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ 123 –ì–ö –†–§"

–í–æ–∑–≤—Ä–∞—â–∞–π —Å—Ç—Ä–æ–≥–æ JSON —Å –ø–æ–ª—è–º–∏:
- label: "task" –∏–ª–∏ "question"
- confidence: —á–∏—Å–ª–æ –æ—Ç 0.0 –¥–æ 1.0 (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
- rationale: –∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç —Å few-shot –ø—Ä–∏–º–µ—Ä–∞–º–∏
    messages = [SystemMessage(content=system_content)]
    for human_msg, ai_msg in few_shot_examples:
        messages.append(human_msg)
        messages.append(ai_msg)
    messages.append(HumanMessage(content=f"–ó–∞–ø—Ä–æ—Å: {question}"))
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º structured output –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        if hasattr(llm, 'with_structured_output'):
            try:
                structured_llm = llm.with_structured_output(ClassificationResult, include_raw=True)
                response = structured_llm.invoke(messages)
                
                if hasattr(response, 'parsed') and response.parsed:
                    classification = response.parsed
                elif isinstance(response, dict) and 'parsed' in response:
                    classification = response['parsed']
                elif isinstance(response, ClassificationResult):
                    classification = response
                else:
                    # Fallback: –ø–∞—Ä—Å–∏–º raw –æ—Ç–≤–µ—Ç
                    raw_content = getattr(response, 'raw', None) or (response.get('raw') if isinstance(response, dict) else str(response))
                    logger.warning(f"Structured output parsing failed, using raw response: {raw_content}")
                    raise ValueError("Failed to parse structured output")
                
                label = classification.label
                confidence = classification.confidence
                rationale = classification.rationale or ""
                
            except Exception as structured_error:
                logger.warning(f"Structured output failed: {structured_error}, falling back to JSON parsing")
                # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π –≤—ã–∑–æ–≤ –∏ –ø–∞—Ä—Å–∏–Ω–≥ JSON
                response = llm.invoke(messages)
                response_text = response.content if hasattr(response, 'content') else str(response)
                
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
                import json
                try:
                    # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
                    json_match = re.search(r'\{[^}]+\}', response_text)
                    if json_match:
                        result_dict = json.loads(json_match.group())
                        label = result_dict.get("label", "question")
                        confidence = float(result_dict.get("confidence", 0.5))
                        rationale = result_dict.get("rationale", "")
                    else:
                        raise ValueError("No JSON found in response")
                except Exception as json_error:
                    logger.error(f"JSON parsing failed: {json_error}, response: {response_text}")
                    # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback: –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥
                    response_lower = response_text.lower()
                    if "task" in response_lower and response_lower.find("task") < response_lower.find("question", response_lower.find("task")):
                        label = "task"
                        confidence = 0.5
                    else:
                        label = "question"
                        confidence = 0.5
                    rationale = ""
        else:
            # LLM –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç structured output, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –≤—ã–∑–æ–≤
            response = llm.invoke(messages)
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            import json
            try:
                json_match = re.search(r'\{[^}]+\}', response_text)
                if json_match:
                    result_dict = json.loads(json_match.group())
                    label = result_dict.get("label", "question")
                    confidence = float(result_dict.get("confidence", 0.5))
                    rationale = result_dict.get("rationale", "")
                else:
                    raise ValueError("No JSON found in response")
            except Exception:
                response_lower = response_text.lower()
                if "task" in response_lower:
                    label = "task"
                    confidence = 0.5
                else:
                    label = "question"
                    confidence = 0.5
                rationale = ""
        
        # 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ confidence threshold
        if confidence >= 0.85:
            # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - –ø—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            final_label = label
        elif confidence >= 0.6:
            # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - –ø—Ä–∏–Ω–∏–º–∞–µ–º, –Ω–æ –ª–æ–≥–∏—Ä—É–µ–º
            final_label = label
            logger.info(f"Medium confidence classification: '{question[:50]}...' -> {label} (confidence: {confidence:.2f})")
        else:
            # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - fallback –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω—É—é –º–µ—Ç–∫—É
            final_label = "question"
            logger.warning(f"Low confidence classification for '{question[:50]}...': {label} (confidence: {confidence:.2f}), falling back to 'question'")
            rationale = f"Low confidence ({confidence:.2f}), fallback to question"
        
        # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
        cache_data = {
            "label": final_label,
            "confidence": confidence,
            "rationale": rationale
        }
        cache.set("classification", normalized_question, cache_data, ttl=3600)
        
        # 7. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        logger.info(f"Classified '{question[:50]}...' as {final_label.upper()} (confidence: {confidence:.2f}, rationale: {rationale[:50] if rationale else 'N/A'})")
        return final_label == "task"
        
    except Exception as e:
        logger.error(f"Error in LLM classification: {e}", exc_info=True)
        logger.warning("LLM classification failed, defaulting to QUESTION")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—à–∏–±–∫—É –≤ –∫—ç—à —Å –∫–æ—Ä–æ—Ç–∫–∏–º TTL, —á—Ç–æ–±—ã –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å –Ω–µ—É–¥–∞—á–Ω—ã–µ –≤—ã–∑–æ–≤—ã
        cache_data = {"label": "question", "confidence": 0.5, "rationale": f"Error: {str(e)[:50]}"}
        cache.set("classification", normalized_question, cache_data, ttl=60)
        return False


async def stream_chat_response(
    case_id: str,
    question: str,
    db: Session,
    current_user: User,
    background_tasks: BackgroundTasks,
    web_search: bool = False,
    legal_research: bool = False,
    deep_think: bool = False
) -> AsyncGenerator[str, None]:
    """
    Stream chat response using RAG and LLM with optional web search and legal research
    
    Yields:
        JSON strings in assistant-ui format
    """
    try:
        # Verify case ownership
        case = db.query(Case).filter(
            Case.id == case_id,
            Case.user_id == current_user.id
        ).first()
        
        if not case:
            yield f"data: {json.dumps({'error': '–î–µ–ª–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'})}\n\n"
            return
        
        # Verify case has files uploaded
        file_count = db.query(FileModel).filter(FileModel.case_id == case_id).count()
        if file_count == 0:
            yield f"data: {json.dumps({'error': '–í –¥–µ–ª–µ –Ω–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.'})}\n\n"
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –ë–î
        import uuid
        from datetime import datetime, timedelta
        user_message_id = str(uuid.uuid4())
        assistant_message_id = None
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º session_id: –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–≤ —Ç–µ—á–µ–Ω–∏–µ 30 –º–∏–Ω—É—Ç), –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö session_id
        # –ò–Ω–∞—á–µ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
        session_id = None
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ –¥–µ–ª–∞
            last_message = db.query(ChatMessage).filter(
                ChatMessage.case_id == case_id,
                ChatMessage.content.isnot(None),
                ChatMessage.content != ""
            ).order_by(ChatMessage.created_at.desc()).first()
            
            if last_message and last_message.created_at:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–æ—à–ª–æ –ª–∏ –º–µ–Ω–µ–µ 30 –º–∏–Ω—É—Ç —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
                time_diff = datetime.utcnow() - last_message.created_at
                if time_diff < timedelta(minutes=30) and last_message.session_id:
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–µ–∫—É—â—É—é —Å–µ—Å—Å–∏—é
                    session_id = last_message.session_id
                    logger.info(f"Continuing existing session {session_id} for case {case_id}")
                else:
                    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
                    session_id = str(uuid.uuid4())
                    logger.info(f"Creating new session {session_id} for case {case_id}")
            else:
                # –ü–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –¥–µ–ª–µ - —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
                session_id = str(uuid.uuid4())
                logger.info(f"Creating first session {session_id} for case {case_id}")
        except Exception as session_error:
            logger.warning(f"Error determining session_id: {session_error}, creating new session")
            session_id = str(uuid.uuid4())
        
        try:
            user_message = ChatMessage(
                id=user_message_id,
                case_id=case_id,
                role="user",
                content=question,
                session_id=session_id
            )
            db.add(user_message)
            
            # –°–æ–∑–¥–∞—ë–º placeholder –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –î–û streaming
            # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ –ø–æ—Å–ª–µ streaming
            assistant_message_id = str(uuid.uuid4())
            assistant_message_placeholder = ChatMessage(
                id=assistant_message_id,
                case_id=case_id,
                role="assistant",
                content="",  # –ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç, –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—ë–Ω –ø–æ—Å–ª–µ streaming
                source_references=None,
                session_id=session_id
            )
            db.add(assistant_message_placeholder)
            db.commit()
            logger.info(f"User message saved to DB with id: {user_message_id}, assistant placeholder: {assistant_message_id}, session: {session_id}")
        except Exception as save_error:
            db.rollback()
            logger.error(f"Error saving messages to DB: {save_error}", exc_info=True)
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è - –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞
        
        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        full_response_text = ""
        sources_list = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ RAG - —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ PipelineService
        logger.info(f"Processing RAG query for case {case_id}: {question[:100]}...")
        
        # asyncio —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≥–ª–æ–±–∞–ª—å–Ω–æ
        loop = asyncio.get_event_loop()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ - –¢–û–õ–¨–ö–û –∏–∑ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏
        chat_history = []
        try:
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ session_id, —á—Ç–æ–±—ã –ò–ò –≤–∏–¥–µ–ª —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∏–π —á–∞—Ç, –∞ –Ω–µ –≤—Å–µ —á–∞—Ç—ã –≤ –¥–µ–ª–µ
            history_query = db.query(ChatMessage).filter(
                ChatMessage.case_id == case_id,
                ChatMessage.content.isnot(None),
                ChatMessage.content != ""
            )
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å session_id, –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏
            if session_id:
                history_query = history_query.filter(ChatMessage.session_id == session_id)
            
            history_messages = history_query.order_by(ChatMessage.created_at.desc()).limit(10).all()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ (–æ—Ç —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º)
            chat_history = []
            for msg in reversed(history_messages):
                if msg.role == "user" and msg.content:
                    chat_history.append(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {msg.content}")
                elif msg.role == "assistant" and msg.content:
                    chat_history.append(f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {msg.content[:500]}...")  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            
            if chat_history:
                logger.info(f"Loaded {len(chat_history)} previous messages for context from session {session_id}")
            else:
                logger.info(f"No previous messages found for context in session {session_id}")
        except Exception as history_error:
            logger.warning(f"Failed to load chat history: {history_error}, continuing without history")
            # Continue without history - –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞
        
        # Web search integration - –≤—ã–ø–æ–ª–Ω—è–µ–º –ü–ï–†–í–´–ú, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
        web_search_context = ""
        web_search_successful = False
        
        if web_search:
            try:
                logger.info(f"Web search enabled for query: {question[:100]}...")
                web_research_service = get_web_research_service()
                research_result = await web_research_service.research(
                    query=question,
                    max_results=5,
                    use_cache=True,
                    validate_sources=True
                )
                
                if research_result.sources:
                    web_search_parts = []
                    web_search_parts.append(f"\n\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–±-–ø–æ–∏—Å–∫–∞ ===")
                    web_search_parts.append(f"–ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(research_result.sources)}")
                    
                    for i, source in enumerate(research_result.sources[:5], 1):
                        title = source.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
                        url = source.get("url", "")
                        content = source.get("content", "")
                        if content:
                            web_search_parts.append(f"\n[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {title}]")
                            if url:
                                web_search_parts.append(f"URL: {url}")
                            web_search_parts.append(f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {content[:300]}...")
                    
                    web_search_context = "\n".join(web_search_parts)
                    web_search_successful = True
                    logger.info(f"Web search completed: {len(research_result.sources)} sources found")
                else:
                    logger.warning("Web search returned no results")
            except Exception as web_search_error:
                logger.warning(f"Web search failed: {web_search_error}, continuing without web search")
                # Continue without web search - –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞
        
        # Legal research integration - –ø–æ–∏—Å–∫ –≤ –ì–ê–†–ê–ù–¢ —Å –∞–Ω–∞–ª–∏–∑–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        legal_research_context = ""
        legal_research_successful = False
        aggregated = []  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç–∞–Ω–æ–≤–∫–µ —Å—Å—ã–ª–æ–∫
        source_router = None  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç–∞–Ω–æ–≤–∫–µ —Å—Å—ã–ª–æ–∫
        
        if legal_research:
            try:
                logger.info(f"Legal research enabled for query: {question[:100]}...")
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º source_router —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
                source_router = initialize_source_router(rag_service=rag_service, register_official_sources=True)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ - —Ç–æ–ª—å–∫–æ –ì–ê–†–ê–ù–¢
                sources_to_search = ["garant"]
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                filters = {}
                
                # –£–º–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –∑–∞–ø—Ä–æ—Å–µ
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
                question_lower = question.lower()
                detected_type = None
                
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –°—É–¥–µ–±–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –∏ –∞–∫—Ç—ã (—Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –∑–∞–ø—Ä–æ—Å)
                court_keywords = [
                    "—Å—É–¥–µ–±–Ω", "—Ä–µ—à–µ–Ω", "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω", "–ø—Ä–∏–≥–æ–≤–æ—Ä", 
                    "–∫–∞—Å—Å–∞—Ü", "–∞–ø–µ–ª–ª—è—Ü", "—Å—É–¥–µ–±–Ω—ã–π –∞–∫—Ç", "—Å—É–¥–µ–±–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ",
                    "—Ä–µ—à–µ–Ω–∏–µ —Å—É–¥–∞", "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É–¥–∞", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É–¥–∞",
                    "–ø—Ä–∞–∫—Ç–∏–∫–∞ —Å—É–¥", "—Å—É–¥–µ–±–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞", "–ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç"
                ]
                if any(keyword in question_lower for keyword in court_keywords):
                    detected_type = "court_decision"
                    logger.info("Detected court decision search, applying doc_type filter")
                
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –ó–∞–∫–æ–Ω—ã –∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∞–∫—Ç—ã
                elif any(keyword in question_lower for keyword in [
                    "–∑–∞–∫–æ–Ω", "–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω", "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤", "–ø—Ä–∏–∫–∞–∑ –º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤",
                    "—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω –∑–∞–∫–æ–Ω", "–∫–æ–¥–µ–∫—Å", "—É–∫–∞–∑ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω",
                    "–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –∞–∫—Ç", "–ø–æ–¥–∑–∞–∫–æ–Ω–Ω—ã–π –∞–∫—Ç"
                ]):
                    detected_type = "law"
                    logger.info("Detected law/regulation search, applying doc_type filter")
                
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –°—Ç–∞—Ç—å–∏ –∫–æ–¥–µ–∫—Å–æ–≤ –∏ –∑–∞–∫–æ–Ω–æ–≤ (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏)
                elif any(keyword in question_lower for keyword in [
                    "—Å—Ç–∞—Ç—å—è", "—Å—Ç.", "–ø—É–Ω–∫—Ç", "—á–∞—Å—Ç—å —Å—Ç–∞—Ç—å–∏", "—Å—Ç–∞—Ç—å—è –∫–æ–¥–µ–∫—Å",
                    "—Å—Ç ", "–ø. ", "—á. ", "—á–∞—Å—Ç—å ", "–ø—É–Ω–∫—Ç —Å—Ç–∞—Ç—å–∏"
                ]):
                    detected_type = "article"
                    logger.info("Detected article search, applying doc_type filter")
                
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 4: –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏—è
                elif any(keyword in question_lower for keyword in [
                    "–∫–æ–º–º–µ–Ω—Ç–∞—Ä", "—Ä–∞–∑—ä—è—Å–Ω–µ–Ω", "–ø–æ–∑–∏—Ü–∏—è –≤–µ—Ä—Ö–æ–≤–Ω", "–æ–±–∑–æ—Ä –ø—Ä–∞–∫—Ç–∏–∫",
                    "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–ª–µ–Ω—É–º", "–ø–ª–µ–Ω—É–º –≤–µ—Ä—Ö–æ–≤–Ω", "—Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏—è –ø–ª–µ–Ω—É–º"
                ]):
                    detected_type = "commentary"
                    logger.info("Detected commentary search, applying doc_type filter")
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç–∏–ø –æ–ø—Ä–µ–¥–µ–ª–µ–Ω —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
                if detected_type:
                    filters["doc_type"] = detected_type
                else:
                    # –ï—Å–ª–∏ —Ç–∏–ø –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω, –∏—â–µ–º –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤ - —ç—Ç–æ –¥–∞—Å—Ç –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    # –ì–ê–†–ê–ù–¢ —Å–∞–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                    logger.info("Document type not detected, searching without filters for broader results")
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                # –î–ª—è —Å—Ç–∞—Ç–µ–π, —Ä–µ—à–µ–Ω–∏–π —Å—É–¥–∞ –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
                need_full_text = any(keyword in question.lower() for keyword in [
                    "—Å—Ç–∞—Ç—å—è", "—Å—Ç.", "–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç", "—Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏", 
                    "—Ç–µ–∫—Å—Ç —Ä–µ—à–µ–Ω–∏—è", "–ø–æ–ª–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ", "–ø—Ä–∏–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç",
                    "–ø–æ–∫–∞–∂–∏ —Ç–µ–∫—Å—Ç", "–≤—ã–ø–∏—à–∏ —Ç–µ–∫—Å—Ç"
                ])
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ source router —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –æ—Ö–≤–∞—Ç–∞
                search_results = await source_router.search(
                    query=question,
                    source_names=sources_to_search,
                    max_results_per_source=20,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (API –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ 20 –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É)
                    filters=filters if filters else None,
                    parallel=True
                )
                
                # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –ì–ê–†–ê–ù–¢, –ø–æ–ª—É—á–∞–µ–º –µ–≥–æ
                if need_full_text and "garant" in search_results:
                    garant_source = source_router._sources.get("garant")
                    if garant_source:
                        garant_results = search_results["garant"]
                        logger.info(f"[Legal Research] Getting full text for {len(garant_results)} Garant documents (requested: need_full_text={need_full_text})")
                        
                        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–∞ Render (–∏–∑–±–µ–≥–∞–µ–º —Ç–∞–π–º–∞—É—Ç–æ–≤)
                        from app.services.external_sources.garant_source import MAX_FULL_TEXT_DOCS, MAX_CONTENT_LENGTH
                        max_full_text_docs = MAX_FULL_TEXT_DOCS
                        for i, result in enumerate(garant_results[:max_full_text_docs], 1):
                            doc_id = result.metadata.get("doc_id")
                            if doc_id:
                                try:
                                    logger.info(f"[Legal Research] Fetching full text for document {i}/{max_full_text_docs}: doc_id={doc_id}")
                                    full_text = await garant_source.get_document_full_text(doc_id, format="html")
                                    if full_text:
                                        # –ü–∞—Ä—Å–∏–º HTML –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                                        try:
                                            from bs4 import BeautifulSoup
                                            soup = BeautifulSoup(full_text, 'html.parser')
                                            text_content = soup.get_text(separator='\n', strip=True)
                                            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–∞ Render
                                            result.content = text_content[:MAX_CONTENT_LENGTH]
                                            logger.info(f"[Legal Research] Got full text for document {doc_id}, extracted: {len(text_content)} chars, using: {len(result.content)} chars")
                                        except ImportError:
                                            # –ï—Å–ª–∏ BeautifulSoup –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –æ—á–∏—Å—Ç–∫—É HTML
                                            import re
                                            text_content = re.sub(r'<[^>]+>', '', full_text)
                                            result.content = text_content[:MAX_CONTENT_LENGTH]
                                            logger.info(f"[Legal Research] Got full text for document {doc_id} (without BeautifulSoup), length: {len(result.content)}")
                                    else:
                                        logger.warning(f"[Legal Research] Failed to get full text for document {doc_id} (API returned None)")
                                except Exception as e:
                                    logger.warning(f"[Legal Research] Error getting full text for document {doc_id}: {e}", exc_info=True)
                        logger.info(f"[Legal Research] Finished fetching full text for documents")
                
                # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã - –±–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                aggregated = source_router.aggregate_results(
                    search_results,
                    max_total=20,  # –ë–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    dedup_threshold=0.85  # –ù–µ–º–Ω–æ–≥–æ —Å–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                )
                
                if aggregated:
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ –≤–æ–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                    legal_research_parts = []
                    legal_research_parts.append(f"\n\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ –ì–ê–†–ê–ù–¢ ===")
                    legal_research_parts.append(f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}")
                    legal_research_parts.append(f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(aggregated)}")
                    legal_research_parts.append("\n–í–ê–ñ–ù–û: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞.")
                    
                    for i, result in enumerate(aggregated[:15], 1):  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 15 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                        title = result.title or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
                        url = result.url or ""
                        content = result.content[:2000] if result.content else ""  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 2000 —Å–∏–º–≤–æ–ª–æ–≤
                        source_name = result.source_name or "garant"
                        relevance = getattr(result, 'relevance_score', 0.5)
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                        metadata = getattr(result, 'metadata', {}) or {}
                        doc_type = metadata.get('doc_type', '')
                        doc_date = metadata.get('doc_date', '')
                        doc_number = metadata.get('doc_number', '')
                        issuing_authority = metadata.get('issuing_authority', '')
                        
                        if content or title:
                            legal_research_parts.append(f"\n{'='*60}")
                            legal_research_parts.append(f"–î–û–ö–£–ú–ï–ù–¢ {i} –ò–ó –ì–ê–†–ê–ù–¢")
                            legal_research_parts.append(f"{'='*60}")
                            legal_research_parts.append(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {title}")
                            
                            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–æ–∫—É —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                            meta_info = []
                            if doc_type:
                                meta_info.append(f"–¢–∏–ø: {doc_type}")
                            if doc_date:
                                meta_info.append(f"–î–∞—Ç–∞: {doc_date}")
                            if doc_number:
                                meta_info.append(f"–ù–æ–º–µ—Ä: {doc_number}")
                            if issuing_authority:
                                meta_info.append(f"–û—Ä–≥–∞–Ω: {issuing_authority}")
                            if meta_info:
                                legal_research_parts.append(" | ".join(meta_info))
                            
                            legal_research_parts.append(f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance:.2%} ({relevance:.2f})")
                            
                            if url:
                                legal_research_parts.append(f"–°—Å—ã–ª–∫–∞ –≤ –ì–ê–†–ê–ù–¢: {url}")
                            
                            if content:
                                legal_research_parts.append(f"\n–°–û–î–ï–†–ñ–ê–ù–ò–ï –î–û–ö–£–ú–ï–ù–¢–ê:")
                                legal_research_parts.append(f"{content}")
                                if len(result.content) > 2000:
                                    legal_research_parts.append(f"\n[... –¥–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–µ–∑–∞–Ω, –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ —Å—Å—ã–ª–∫–µ –≤—ã—à–µ ...]")
                            else:
                                legal_research_parts.append(f"\n–î–æ—Å—Ç—É–ø–µ–Ω —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
                            
                            legal_research_parts.append(f"{'='*60}\n")
                    
                    legal_research_context = "\n".join(legal_research_parts)
                    legal_research_successful = True
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ sources_list
                    for result in aggregated[:10]:
                        source_info = {
                            "title": result.title or "–ì–ê–†–ê–ù–¢",
                            "url": result.url or "",
                            "source": "garant"
                        }
                        if result.content:
                            source_info["text_preview"] = result.content[:200]
                        sources_list.append(source_info)
                    
                    logger.info(f"Legal research completed: {len(aggregated)} sources found from –ì–ê–†–ê–ù–¢")
                else:
                    logger.warning("Legal research returned no results from –ì–ê–†–ê–ù–¢")
            except Exception as legal_research_error:
                logger.warning(f"Legal research failed: {legal_research_error}, continuing without legal research", exc_info=True)
                # Continue without legal research - –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞
        
        # Get relevant documents using RAG - –í–°–ï–ì–î–ê –≤—ã–ø–æ–ª–Ω—è–µ–º RAG –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–µ–ª–∞
        # –í–µ–±-–ø–æ–∏—Å–∫ –∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω—è—é—Ç, –Ω–æ –Ω–µ –∑–∞–º–µ–Ω—è—é—Ç RAG
        context = ""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ rag_service –≤ lambda
            documents = await loop.run_in_executor(
                None,
                lambda rs=rag_service: rs.retrieve_context(
                    case_id=case_id,
                    query=question,
                    k=10,
                    db=db
                )
            )
            
            # Build context from documents and collect sources using RAGService.format_sources
            context_parts = []
            if documents:
                # Use RAGService.format_sources for consistent source formatting
                formatted_sources = rag_service.format_sources(documents[:5])
                sources_list.extend(formatted_sources)
                
                # Build context from documents
                for i, doc in enumerate(documents[:5], 1):
                    if hasattr(doc, 'page_content'):
                        content = doc.page_content[:500] if doc.page_content else ""
                        source = doc.metadata.get("source_file", "unknown") if hasattr(doc, 'metadata') and doc.metadata else "unknown"
                    elif isinstance(doc, dict):
                        content = doc.get("content", "")[:500]
                        source = doc.get("file", "unknown")
                    else:
                        continue
                    
                    context_parts.append(f"[–î–æ–∫—É–º–µ–Ω—Ç {i}: {source}]\n{content}")
                
                context = "\n\n".join(context_parts)
                if context:
                    logger.info(f"RAG retrieved {len(documents)} documents for context, {len(sources_list)} sources formatted")
                else:
                    logger.warning(f"RAG retrieved {len(documents)} documents but context is empty")
        except Exception as rag_error:
            logger.warning(f"RAG retrieval failed: {rag_error}, continuing without RAG context", exc_info=True)
            # Continue without RAG context - –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞, –Ω–æ –≤–∞–∂–Ω–æ –¥–ª—è deep_think
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –≤–µ–±-–ø–æ–∏—Å–∫–∞
        if web_search_successful and 'research_result' in locals() and research_result.sources:
            for source in research_result.sources[:5]:
                source_info = {
                    "title": source.get("title", "–í–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫"),
                    "url": source.get("url", ""),
                }
                if source.get("content"):
                    source_info["text_preview"] = source.get("content", "")[:200]
                sources_list.append(source_info)
        
        # Create prompt
        web_search_instructions = ""
        if web_search_context:
            web_search_instructions = """
–ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –í–ï–ë-–ü–û–ò–°–ö–ê:
- –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤–µ–±-–ø–æ–∏—Å–∫–∞ –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞, –∫–æ–≥–¥–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–µ–ª–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
- –ü—Ä–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –≤–µ–±-–ø–æ–∏—Å–∫–∞ —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫ (–Ω–∞–∑–≤–∞–Ω–∏–µ –∏ URL –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
- –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–µ–ª–∞, –≤–µ–±-–ø–æ–∏—Å–∫ - –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –≤–µ–±-–ø–æ–∏—Å–∫–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –¥–µ–ª–∞, —É–∫–∞–∂–∏ —ç—Ç–æ –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–µ–ª–∞
"""

        legal_research_instructions = ""
        if legal_research_context:
            legal_research_instructions = """
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ü–û–ò–°–ö–ê –í –ì–ê–†–ê–ù–¢
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

–¢—ã –ø–æ–ª—É—á–∏–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ –ì–ê–†–ê–ù–¢ - —ç—Ç–æ –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–ô –ò–°–¢–û–ß–ù–ò–ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏!

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ü–†–ê–í–ò–õ–ê:
1. –ù–ï –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã - –ü–†–û–ê–ù–ê–õ–ò–ó–ò–†–£–ô –∏—Ö –∏ –¥–∞–π –ü–†–Ø–ú–û–ô –û–¢–í–ï–¢ –Ω–∞ –≤–æ–ø—Ä–æ—Å
2. –ò—Å–ø–æ–ª—å–∑—É–π –ö–û–ù–ö–†–ï–¢–ù–´–ï –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ì–ê–†–ê–ù–¢: –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –¥–∞—Ç—ã, –Ω–æ–º–µ—Ä–∞, —Ü–∏—Ç–∞—Ç—ã
3. –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –≤—ã—Å–æ–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é (–ø–æ–∫–∞–∑–∞–Ω–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö)
4. –ü—Ä–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –í–°–ï–ì–î–ê —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫: [–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞](URL –∏–∑ –ì–ê–†–ê–ù–¢)

–î–õ–Ø –†–ê–ó–ù–´–• –¢–ò–ü–û–í –ó–ê–ü–†–û–°–û–í:

üìã –°—É–¥–µ–±–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è:
- –ü–†–ò–í–ï–î–ò –ö–û–ù–ö–†–ï–¢–ù–´–ï –†–ï–®–ï–ù–ò–Ø —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏, –¥–∞—Ç–∞–º–∏, –Ω–æ–º–µ—Ä–∞–º–∏ –¥–µ–ª
- –£–∫–∞–∂–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã –∏ –ø—Ä–∞–≤–æ–≤—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è
- –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–µ—à–µ–Ω–∏–π - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –∏—Ö –ø–æ –¥–∞—Ç–∞–º –∏–ª–∏ —Ç–µ–º–∞–º

üìú –ó–∞–∫–æ–Ω—ã –∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∞–∫—Ç—ã:
- –ü—Ä–∏–≤–µ–¥–∏ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –£–∫–∞–∂–∏ –Ω–æ–º–µ—Ä, –¥–∞—Ç—É –ø—Ä–∏–Ω—è—Ç–∏—è, –æ—Ä–≥–∞–Ω, –ø—Ä–∏–Ω—è–≤—à–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
- –ï—Å–ª–∏ –µ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è - —É–∫–∞–∂–∏ –∞–∫—Ç—É–∞–ª—å–Ω—É—é —Ä–µ–¥–∞–∫—Ü–∏—é

üìñ –°—Ç–∞—Ç—å–∏ –∫–æ–¥–µ–∫—Å–æ–≤:
- –ù–∞–π–¥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å—Ç–∞—Ç—å—é –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –∏ –ø—Ä–∏–≤–µ–¥–∏ –µ—ë –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
- –£–∫–∞–∂–∏ –ø—É–Ω–∫—Ç—ã –∏ —á–∞—Å—Ç–∏ —Å—Ç–∞—Ç—å–∏, –µ—Å–ª–∏ –æ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã
- –ü—Ä–∏–≤–µ–¥–∏ —Å—Å—ã–ª–∫—É –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç –≤ –ì–ê–†–ê–ù–¢

üí° –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏—è:
- –ò—Å–ø–æ–ª—å–∑—É–π —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏—è –¥–ª—è —Ç–æ–ª–∫–æ–≤–∞–Ω–∏—è –Ω–æ—Ä–º –ø—Ä–∞–≤–∞
- –ü—Ä–∏–≤–µ–¥–∏ –ø–æ–∑–∏—Ü–∏–∏ –í–µ—Ä—Ö–æ–≤–Ω–æ–≥–æ –°—É–¥–∞ –∏–ª–∏ –ü–ª–µ–Ω—É–º–æ–≤
- –£–∫–∞–∂–∏, –∫–∞–∫ —ç—Ç–æ –ø—Ä–∏–º–µ–Ω–∏–º–æ –∫ –≤–æ–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

‚ö†Ô∏è –ï–°–õ–ò –ò–ù–§–û–†–ú–ê–¶–ò–ò –ù–ï–î–û–°–¢–ê–¢–û–ß–ù–û:
- –ß–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏, —á—Ç–æ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ì–ê–†–ê–ù–¢ –Ω–µ—Ç –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π –æ—Ç–≤–µ—Ç - –ª—É—á—à–µ –ø—Ä–∏–∑–Ω–∞—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
- –ü—Ä–µ–¥–ª–æ–∂–∏ —É—Ç–æ—á–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –ø–æ–∏—Å–∫–∞—Ç—å –≤ –¥—Ä—É–≥–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö

–°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê:
1. –ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
2. –î–µ—Ç–∞–ª–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ì–ê–†–ê–ù–¢ —Å —Ü–∏—Ç–∞—Ç–∞–º–∏ –∏ —Å—Å—ã–ª–∫–∞–º–∏
3. –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

–ù–ï –ò–ì–ù–û–†–ò–†–£–ô –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ò–ó –ì–ê–†–ê–ù–¢ - –æ–Ω–∏ —è–≤–ª—è—é—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º!
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        history_context = ""
        if chat_history:
            history_context = f"""
–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —ç—Ç–æ–º —á–∞—Ç–µ:
{chr(10).join(chat_history)}

–í–ê–ñ–ù–û: –£—á–∏—Ç—ã–≤–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ. –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–µ—Ç —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ø–æ–¥—Ä–æ–±–Ω–µ–µ"), –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.
"""

        prompt = f"""–¢—ã - —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –¢—ã –ø–æ–º–æ–≥–∞–µ—à—å –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–µ–ª–∞.

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–µ–ª–∞:
{context if context else "–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–µ–ª–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥—Ä—É–≥–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."}{web_search_context}{legal_research_context}{history_context}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}{web_search_instructions}{legal_research_instructions}

–í–ê–ñ–ù–û - –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï –û–¢–í–ï–¢–ê:
1. –í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–π Markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤:
   - **–∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç** –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
   - *–∫—É—Ä—Å–∏–≤* –¥–ª—è –∞–∫—Ü–µ–Ω—Ç–æ–≤
   - –ó–∞–≥–æ–ª–æ–≤–∫–∏ (##, ###) –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
   - –°–ø–∏—Å–∫–∏ (- –∏–ª–∏ 1.) –¥–ª—è –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–π

2. –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –§–û–†–ú–ê–¢ –°–°–´–õ–û–ö –ù–ê –î–û–ö–£–ú–ï–ù–¢–´ (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û!):
   –ü—Ä–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–µ–ª–∞ –í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Ñ–æ—Ä–º–∞—Ç [1], [2], [3] –∏ —Ç.–¥.
   - –ü–µ—Ä–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ = [1]
   - –í—Ç–æ—Ä–æ–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ = [2]
   - –¢—Ä–µ—Ç–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ = [3]
   - –ó–∞–ø—Ä–µ—â–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: [Document 1], [–î–æ–∫—É–º–µ–Ω—Ç 1], [–î–æ–∫—É–º–µ–Ω—Ç: filename.pdf], [–î–æ–∫—É–º–µ–Ω—Ç 1: ...]
   - –†–∞–∑—Ä–µ—à–µ–Ω –¢–û–õ–¨–ö–û —Ñ–æ—Ä–º–∞—Ç: [1], [2], [3] - —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö
   - –ü—Ä–∏–º–µ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞: "–°–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –¥–µ–ª–∞ [1][2], —Å—Ç–æ—Ä–æ–Ω—ã –æ–±—è–∑–∞–Ω—ã..."
   - –ü—Ä–∏–º–µ—Ä –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ (–∑–∞–ø—Ä–µ—â–µ–Ω): "–°–æ–≥–ª–∞—Å–Ω–æ [Document 1] –∏ [–î–æ–∫—É–º–µ–Ω—Ç 2]..."
   –ó–ê–ü–û–ú–ù–ò: –¢–û–õ–¨–ö–û [1], [2], [3] - –Ω–∏–∫–∞–∫–∏—Ö –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤!

3. –ï–°–õ–ò –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç —Å–æ–∑–¥–∞—Ç—å –¢–ê–ë–õ–ò–¶–£:
   - –í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–π Markdown —Ç–∞–±–ª–∏—Ü—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
   | –ö–æ–ª–æ–Ω–∫–∞ 1 | –ö–æ–ª–æ–Ω–∫–∞ 2 | –ö–æ–ª–æ–Ω–∫–∞ 3 |
   |-----------|-----------|-----------|
   | –î–∞–Ω–Ω—ã–µ 1  | –î–∞–Ω–Ω—ã–µ 2  | –î–∞–Ω–Ω—ã–µ 3  |
   
   - –ù–ï –æ—Ç–ø—Ä–∞–≤–ª—è–π —Ç–∞–±–ª–∏—Ü—ã –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç —Å–æ –∑–≤–µ–∑–¥–æ—á–∫–∞–º–∏
   - –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º–∞—Ç "–î–∞—Ç–∞ | –°—É–¥—å—è | –î–æ–∫—É–º–µ–Ω—Ç" –±–µ–∑ markdown —Ç–∞–±–ª–∏—Ü—ã
   - –¢–∞–±–ª–∏—Ü–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ Markdown

4. –î–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–¥–∞—Ç—ã, —Å—É–¥—å–∏, –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Å–æ–±—ã—Ç–∏—è):
   - –í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–π Markdown —Ç–∞–±–ª–∏—Ü—ã
   - –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á–µ—Ç–∫–∏–º–∏
   - –î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ —Å—Ç—Ä–æ–∫–∞—Ö —Ç–∞–±–ª–∏—Ü—ã

5. –ü—Ä–∏–º–µ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å —Ç–∞–±–ª–∏—Ü–µ–π:
   ## –¢–∞–±–ª–∏—Ü–∞ —Å—É–¥–µ–±–Ω—ã—Ö –∑–∞—Å–µ–¥–∞–Ω–∏–π
   
   | –î–∞—Ç–∞ | –°—É–¥—å—è | –ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ |
   |------|-------|-----------------|
   | 22.08.2016 | –ù–µ —É–∫–∞–∑–∞–Ω | A83-6426-2015 |
   | 15.03.2017 | –ï.–ê. –û—Å—Ç–∞–ø–æ–≤ | A83-6426-2015 |

–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–µ–ª–∞. {f"–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–±-–ø–æ–∏—Å–∫–∞ –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞." if web_search_context else ""}{f" –ò—Å–ø–æ–ª—å–∑—É–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –Ω–æ—Ä–º–∞—Ö –ø—Ä–∞–≤–∞ –∏ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ." if legal_research_context else ""}{" –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —É–∫–∞–∂–∏ —ç—Ç–æ." if not web_search_context and not legal_research_context else ""}

–ü–û–í–¢–û–†–Ø–Æ –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û–ï –ü–†–ê–í–ò–õ–û: –ü—Ä–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–µ–ª–∞ –∏—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Ñ–æ—Ä–º–∞—Ç [1], [2], [3] - —á–∏—Å–ª–æ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π [Document 1], [–î–æ–∫—É–º–µ–Ω—Ç 1] –∏–ª–∏ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–∞—Ç!

–ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º. –í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–π Markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.
{f"–ü—Ä–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –≤–µ–±-–ø–æ–∏—Å–∫–∞ —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ: [–ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞](URL) –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞, –µ—Å–ª–∏ URL –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω." if web_search_context else ""}{f" –ü—Ä–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å—Ç–∞—Ç–µ–π –∫–æ–¥–µ–∫—Å–æ–≤ –∏–ª–∏ –Ω–æ—Ä–º –ø—Ä–∞–≤–∞ —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫: [–ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏](URL) –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞." if legal_research_context else ""}"""

        # Initialize LLM
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º create_legal_llm() –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ (temperature=0.0)
        # –ü—Ä–∏ deep_think=True –∏—Å–ø–æ–ª—å–∑—É–µ–º GigaChat-Pro, –∏–Ω–∞—á–µ –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (GigaChat)
        if deep_think:
            llm = create_legal_llm(model="GigaChat-Pro")  # temperature=0.0 –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
            logger.info(f"Using GigaChat-Pro for deep thinking mode (temperature=0.0). Context length: {len(context)} chars, History messages: {len(chat_history)}, Web search: {web_search_successful}, Legal research: {legal_research_successful}")
        else:
            llm = create_legal_llm()  # temperature=0.0 –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç config.GIGACHAT_MODEL (–æ–±—ã—á–Ω–æ "GigaChat")
            logger.info(f"Using standard GigaChat (temperature=0.0). Context length: {len(context)} chars, History messages: {len(chat_history)}")
        
        # Stream response
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É prompt –≤ —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è GigaChat
        from langchain_core.messages import HumanMessage
        messages = [HumanMessage(content=prompt)]
        
        try:
            if hasattr(llm, 'astream'):
                async for chunk in llm.astream(messages):
                    if hasattr(chunk, 'content'):
                        content = chunk.content
                    elif isinstance(chunk, str):
                        content = chunk
                    else:
                        content = str(chunk)
                    
                    full_response_text += content
                    yield f"data: {json.dumps({'textDelta': content}, ensure_ascii=False)}\n\n"
                
                # –ü—Ä–æ—Å—Ç–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ì–ê–†–ê–ù–¢ –≤ –æ—Ç–≤–µ—Ç–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ)
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–∞ Render (–ª–∏–º–∏—Ç API: 20–ú–±, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º 8–ö–ë –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)
                max_text_for_links = 8000  # 8KB –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–∞ Render
                if legal_research_successful and aggregated and source_router and len(full_response_text) < max_text_for_links:
                    try:
                        garant_source = source_router._sources.get("garant")
                        if garant_source:
                            logger.info(f"[Legal Research] Attempting to insert Garant links into response (text length: {len(full_response_text)} chars)")
                            text_with_links = await garant_source.insert_links(full_response_text)
                            if text_with_links and text_with_links != full_response_text:
                                # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∏ –±—ã–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã, –æ–±–Ω–æ–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
                                full_response_text = text_with_links
                                logger.info(f"[Legal Research] Successfully inserted Garant links, new length: {len(full_response_text)} chars")
                            elif text_with_links == full_response_text:
                                logger.info(f"[Legal Research] Link insertion returned same text (no links found or inserted)")
                            else:
                                logger.warning(f"[Legal Research] Link insertion returned None (API error or limit exceeded)")
                    except Exception as e:
                        logger.warning(f"[Legal Research] Failed to insert Garant links: {e}", exc_info=True)
                elif legal_research_successful and len(full_response_text) >= max_text_for_links:
                    logger.info(f"[Legal Research] Skipping link insertion: text too long ({len(full_response_text)} chars, max: {max_text_for_links})")
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ SSE
                if sources_list:
                    logger.info(f"Sending {len(sources_list)} sources via SSE (fallback) for case {case_id}")
                    yield f"data: {json.dumps({'type': 'sources', 'sources': sources_list}, ensure_ascii=False)}\n\n"
                else:
                    logger.warning(f"No sources to send (fallback) for case {case_id}, query: {question[:100]}")
                
                yield f"data: {json.dumps({'textDelta': ''})}\n\n"
            else:
                # Fallback: get full response and chunk it
                response = await loop.run_in_executor(None, lambda: llm.invoke(messages))
                response_text = response.content if hasattr(response, 'content') else str(response)
                full_response_text = response_text
                
                # –ü—Ä–æ—Å—Ç–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ì–ê–†–ê–ù–¢ –≤ –æ—Ç–≤–µ—Ç–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ)
                max_text_for_links = 8000  # 8KB –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–∞ Render
                if legal_research_successful and aggregated and source_router and len(full_response_text) < max_text_for_links:
                    try:
                        garant_source = source_router._sources.get("garant")
                        if garant_source:
                            logger.info(f"[Legal Research] Attempting to insert Garant links (fallback, text length: {len(full_response_text)} chars)")
                            text_with_links = await garant_source.insert_links(full_response_text)
                            if text_with_links and text_with_links != full_response_text:
                                full_response_text = text_with_links
                                response_text = text_with_links
                                logger.info(f"[Legal Research] Successfully inserted Garant links (fallback), new length: {len(full_response_text)} chars")
                            else:
                                logger.info(f"[Legal Research] Link insertion returned same text or None (fallback)")
                    except Exception as e:
                        logger.warning(f"[Legal Research] Failed to insert Garant links (fallback): {e}", exc_info=True)
                elif legal_research_successful and len(full_response_text) >= max_text_for_links:
                    logger.info(f"[Legal Research] Skipping link insertion (fallback): text too long ({len(full_response_text)} chars)")
                
                chunk_size = 20
                for i in range(0, len(response_text), chunk_size):
                    chunk = response_text[i:i + chunk_size]
                    yield f"data: {json.dumps({'textDelta': chunk}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.05)
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ SSE
                if sources_list:
                    logger.info(f"Sending {len(sources_list)} sources via SSE (fallback) for case {case_id}")
                    yield f"data: {json.dumps({'type': 'sources', 'sources': sources_list}, ensure_ascii=False)}\n\n"
                else:
                    logger.warning(f"No sources to send (fallback) for case {case_id}, query: {question[:100]}")
                
                yield f"data: {json.dumps({'textDelta': ''})}\n\n"
        except Exception as stream_error:
            logger.warning(f"Streaming failed, using fallback: {stream_error}")
            response = await loop.run_in_executor(None, lambda: llm.invoke(messages))
            response_text = response.content if hasattr(response, 'content') else str(response)
            full_response_text = response_text
            
            # –ü—Ä–æ—Å—Ç–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ì–ê–†–ê–ù–¢ –≤ –æ—Ç–≤–µ—Ç–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ)
            max_text_for_links = 8000  # 8KB –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–∞ Render
            if legal_research_successful and aggregated and source_router and len(full_response_text) < max_text_for_links:
                try:
                    garant_source = source_router._sources.get("garant")
                    if garant_source:
                        logger.info(f"[Legal Research] Attempting to insert Garant links (error fallback, text length: {len(full_response_text)} chars)")
                        text_with_links = await garant_source.insert_links(full_response_text)
                        if text_with_links and text_with_links != full_response_text:
                            full_response_text = text_with_links
                            response_text = text_with_links
                            logger.info(f"[Legal Research] Successfully inserted Garant links (error fallback), new length: {len(full_response_text)} chars")
                        else:
                            logger.info(f"[Legal Research] Link insertion returned same text or None (error fallback)")
                except Exception as e:
                    logger.warning(f"[Legal Research] Failed to insert Garant links (error fallback): {e}", exc_info=True)
            elif legal_research_successful and len(full_response_text) >= max_text_for_links:
                logger.info(f"[Legal Research] Skipping link insertion (error fallback): text too long ({len(full_response_text)} chars)")
            
            chunk_size = 20
            for i in range(0, len(response_text), chunk_size):
                chunk = response_text[i:i + chunk_size]
                yield f"data: {json.dumps({'textDelta': chunk}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.05)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ SSE
            if sources_list:
                yield f"data: {json.dumps({'type': 'sources', 'sources': sources_list}, ensure_ascii=False)}\n\n"
            
            yield f"data: {json.dumps({'textDelta': ''})}\n\n"
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è streaming
        if assistant_message_id:
            try:
                assistant_message = db.query(ChatMessage).filter(
                    ChatMessage.id == assistant_message_id
                ).first()
                if assistant_message:
                    assistant_message.content = full_response_text
                    assistant_message.source_references = sources_list if sources_list else None
                    db.commit()
                    logger.info(f"Assistant message updated in DB for case {case_id}, id: {assistant_message_id}")
                else:
                    logger.warning(f"Assistant message placeholder {assistant_message_id} not found, creating new one")
                    # Fallback: —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –µ—Å–ª–∏ placeholder –Ω–µ –Ω–∞–π–¥–µ–Ω
                    assistant_message = ChatMessage(
                        case_id=case_id,
                        role="assistant",
                        content=full_response_text,
                        source_references=sources_list if sources_list else None,
                        session_id=session_id
                    )
                    db.add(assistant_message)
                    db.commit()
            except Exception as update_error:
                db.rollback()
                logger.error(f"Error updating assistant message in DB: {update_error}", exc_info=True)
                # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–∞–∫ fallback
                try:
                    assistant_message = ChatMessage(
                        case_id=case_id,
                        role="assistant",
                        content=full_response_text,
                        source_references=sources_list if sources_list else None,
                        session_id=session_id
                    )
                    db.add(assistant_message)
                    db.commit()
                    logger.info(f"Assistant message created as fallback for case {case_id}")
                except Exception as fallback_error:
                    db.rollback()
                    logger.error(f"Error creating fallback assistant message: {fallback_error}", exc_info=True)
    
    except Exception as e:
        logger.error(f"Error in stream_chat_response: {e}", exc_info=True)
        yield f"data: {json.dumps({'error': str(e)})}\n\n"


@router.post("/api/assistant/chat")
async def assistant_chat(
    request: Request,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Streaming chat endpoint for assistant-ui
    
    Uses RAG only - simple question answering without agents.
    Accepts request body with messages array and case_id
    Returns Server-Sent Events (SSE) stream
    """
    try:
        # Parse request body
        body = await request.json()
        messages = body.get("messages", [])
        case_id = body.get("case_id") or body.get("caseId")
        web_search_raw = body.get("web_search", False)
        # Normalize web_search to boolean
        if isinstance(web_search_raw, str):
            web_search = web_search_raw.lower() in ("true", "1", "yes")
        else:
            web_search = bool(web_search_raw)
        legal_research_raw = body.get("legal_research", False)
        # Normalize legal_research to boolean
        if isinstance(legal_research_raw, str):
            legal_research = legal_research_raw.lower() in ("true", "1", "yes")
        else:
            legal_research = bool(legal_research_raw)
        deep_think = body.get("deep_think", False)
        
        if not case_id:
            raise HTTPException(status_code=400, detail="case_id is required")
        
        # Get last user message
        if not messages:
            raise HTTPException(status_code=400, detail="No messages provided")
        
        last_message = messages[-1]
        if last_message.get("role") != "user":
            raise HTTPException(status_code=400, detail="Last message must be from user")
        
        question = last_message.get("content", "")
        
        # Use direct RAG stream_chat_response - no agents, simple question answering
        return StreamingResponse(
            stream_chat_response(
                case_id=case_id,
                question=question,
                db=db,
                current_user=current_user,
                background_tasks=background_tasks,
                web_search=web_search,
                legal_research=legal_research,
                deep_think=deep_think
            ),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in assistant_chat endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/api/assistant/chat/{case_id}/sessions")
async def get_chat_sessions_for_case(
    case_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get list of chat sessions for a specific case
    
    Returns: list of sessions with session_id, first_message, last_message, last_message_at, message_count
    """
    # Check if case exists and verify ownership
    case = db.query(Case).filter(
        Case.id == case_id,
        Case.user_id == current_user.id
    ).first()
    if not case:
        raise HTTPException(status_code=404, detail="–î–µ–ª–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    try:
        from sqlalchemy import func, desc
        
        # Get all unique session_ids for this case
        sessions_query = db.query(
            ChatMessage.session_id,
            func.min(ChatMessage.created_at).label('first_message_at'),
            func.max(ChatMessage.created_at).label('last_message_at'),
            func.count(ChatMessage.id).label('message_count')
        ).filter(
            ChatMessage.case_id == case_id,
            ChatMessage.content.isnot(None),
            ChatMessage.content != "",
            ChatMessage.session_id.isnot(None)
        ).group_by(ChatMessage.session_id).order_by(desc('last_message_at')).all()
        
        sessions = []
        for session_row in sessions_query:
            session_id = session_row.session_id
            
            # Get first and last messages for preview
            first_message = db.query(ChatMessage).filter(
                ChatMessage.case_id == case_id,
                ChatMessage.session_id == session_id,
                ChatMessage.content.isnot(None),
                ChatMessage.content != ""
            ).order_by(ChatMessage.created_at.asc()).first()
            
            last_message = db.query(ChatMessage).filter(
                ChatMessage.case_id == case_id,
                ChatMessage.session_id == session_id,
                ChatMessage.content.isnot(None),
                ChatMessage.content != ""
            ).order_by(ChatMessage.created_at.desc()).first()
            
            first_message_preview = ""
            if first_message and first_message.content:
                first_message_preview = first_message.content[:100]
                if len(first_message.content) > 100:
                    first_message_preview += "..."
            
            last_message_preview = ""
            if last_message and last_message.content:
                last_message_preview = last_message.content[:100]
                if len(last_message.content) > 100:
                    last_message_preview += "..."
            
            sessions.append({
                "session_id": session_id,
                "first_message": first_message_preview,
                "last_message": last_message_preview,
                "first_message_at": first_message.created_at.isoformat() if first_message and first_message.created_at else None,
                "last_message_at": last_message.created_at.isoformat() if last_message and last_message.created_at else None,
                "message_count": session_row.message_count
            })
        
        return {"sessions": sessions}
    except Exception as e:
        logger.error(f"Error getting chat sessions for case {case_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to get chat sessions")


@router.get("/api/assistant/chat/{case_id}/history")
async def get_assistant_chat_history(
    case_id: str,
    session_id: Optional[str] = None,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get chat history for assistant chat
    
    Args:
        case_id: Case identifier
        session_id: Optional session ID to filter messages by session. If not provided, returns all messages for the case.
    
    Returns: list of messages with role, content, sources, created_at, session_id
    """
    # Check if case exists and verify ownership
    case = db.query(Case).filter(
        Case.id == case_id,
        Case.user_id == current_user.id
    ).first()
    if not case:
        raise HTTPException(status_code=404, detail="–î–µ–ª–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    # Get messages - —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –ø–æ session_id –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    query = db.query(ChatMessage).filter(
        ChatMessage.case_id == case_id,
        ChatMessage.content.isnot(None),
        ChatMessage.content != ""
    )
    
    if session_id:
        query = query.filter(ChatMessage.session_id == session_id)
    
    messages = query.order_by(ChatMessage.created_at.asc()).all()
    
    return {
        "messages": [
            {
                "role": msg.role,
                "content": msg.content or "",
                "sources": msg.source_references if msg.source_references is not None else [],
                "created_at": msg.created_at.isoformat() if msg.created_at else datetime.utcnow().isoformat(),
                "session_id": msg.session_id
            }
            for msg in messages
        ]
    }


class HumanFeedbackResponseRequest(BaseModel):
    """Request model for submitting human feedback response"""
    request_id: str = Field(..., description="–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏")
    response: str = Field(..., description="–û—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
    case_id: Optional[str] = Field(None, description="–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–µ–ª–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏)")


@router.post("/api/assistant/chat/human-feedback")
async def submit_human_feedback(
    request: HumanFeedbackResponseRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Submit human feedback response for agent requests
    
    This endpoint receives user responses to agent feedback requests
    (e.g., approval requests, clarification questions, etc.)
    """
    try:
        from app.services.langchain_agents.human_feedback import get_feedback_service
        
        # Get feedback service
        feedback_service = get_feedback_service(db)
        
        # Submit response
        success = feedback_service.receive_response(
            request_id=request.request_id,
            response=request.response,
            run_id=None  # run_id can be extracted from state if needed
        )
        
        if not success:
            logger.warning(f"Failed to submit feedback response for request {request.request_id}: request not found or already answered")
            raise HTTPException(
                status_code=404,
                detail="–ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω"
            )
        
        # Log feedback submission
        if request.case_id:
            from app.services.langchain_agents.audit_logger import get_audit_logger
            audit_logger = get_audit_logger()
            audit_logger.log_human_feedback(
                request_id=request.request_id,
                question="",  # Question is stored in the request itself
                response=request.response,
                case_id=request.case_id,
                user_id=str(current_user.id),
                approved=None  # Can be determined from response if needed
            )
        
        logger.info(f"Human feedback response submitted successfully for request {request.request_id}")
        
        return {
            "status": "success",
            "request_id": request.request_id,
            "message": "–û—Ç–≤–µ—Ç —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error submitting human feedback response: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –æ—Ç–≤–µ—Ç–∞: {str(e)}"
        )


class ResumeGraphRequest(BaseModel):
    """Request model for resuming graph execution after interrupt"""
    thread_id: str = Field(..., description="Thread ID –¥–ª—è resume")
    case_id: str = Field(..., description="–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–µ–ª–∞")
    answer: dict = Field(..., description="–û—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, {'doc_types': ['contract'], 'columns_clarification': '...'})")


@router.post("/api/assistant/chat/resume")
async def resume_graph_execution(
    request: ResumeGraphRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    –í–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –ø–æ—Å–ª–µ interrupt
    
    –≠—Ç–æ—Ç endpoint –ø–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å –æ—Ç –∞–≥–µ–Ω—Ç–∞
    –∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª—è–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —á–µ—Ä–µ–∑ Command(resume=...)
    """
    try:
        from app.services.langchain_agents.coordinator import AgentCoordinator
        from app.services.rag_service import RAGService
        from app.services.document_processor import DocumentProcessor
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–µ–ª–æ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        case = db.query(Case).filter(
            Case.id == request.case_id,
            Case.user_id == current_user.id
        ).first()
        
        if not case:
            raise HTTPException(
                status_code=404,
                detail="–î–µ–ª–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
            )
        
        # –°–æ–∑–¥–∞–µ–º coordinator –¥–ª—è resume
        rag_service = RAGService()
        document_processor = DocumentProcessor()
        coordinator = AgentCoordinator(db, rag_service, document_processor)
        
        # –°–æ–∑–¥–∞–µ–º step_callback –¥–ª—è streaming (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω)
        # –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º
        def step_callback(event):
            logger.debug(f"[Resume] Stream event: {type(event)}")
        
        # –í—ã–∑—ã–≤–∞–µ–º resume
        result = coordinator.resume_after_interrupt(
            thread_id=request.thread_id,
            case_id=request.case_id,
            answer=request.answer,
            step_callback=step_callback
        )
        
        logger.info(f"Graph execution resumed successfully for thread {request.thread_id}")
        
        return {
            "status": "resumed",
            "thread_id": request.thread_id,
            "result": result
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error resuming graph execution: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {str(e)}"
        )

